{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddee37fb-a32c-41b3-85bb-cf782636426b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.94.57.245:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab02_Spark_RDD_Core</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x106198d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"lab02_Spark_RDD_Core\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e17e33-36a7-463a-8c97-3b14c53c98f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abidhiafahmed/projets/realtime-spark/data/iris.csv\n"
     ]
    }
   ],
   "source": [
    "~/projets/realtime-spark/data/kaggle_text/reviews_sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f615f30b-0db1-4cf1-a7bc-65ceec9dbb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: double (nullable = true)\n",
      " |-- SepalWidthCm: double (nullable = true)\n",
      " |-- PetalLengthCm: double (nullable = true)\n",
      " |-- PetalWidthCm: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_path = \"/Users/abidhiafahmed/projets/realtime-spark/data/iris.csv\"\n",
    "\n",
    "df_iris = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(iris_path)\n",
    ")\n",
    "\n",
    "df_iris.printSchema()\n",
    "df_iris.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb11ad7e-76d9-42f0-bd7a-ff7aa6fc6914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|        species|\n",
      "+---------------+\n",
      "| Iris-virginica|\n",
      "|    Iris-setosa|\n",
      "|Iris-versicolor|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  df_iris.count()\n",
    "  \n",
    "  df_iris.select(\"species\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e3009da-08c5-4cf5-b368-ee0526e71517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|        species| avg_sepal_length|\n",
      "+---------------+-----------------+\n",
      "| Iris-virginica|6.587999999999998|\n",
      "|    Iris-setosa|5.005999999999999|\n",
      "|Iris-versicolor|            5.936|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_iris.groupBy(\"species\").agg(avg(\"SepalLengthCm\").alias(\"avg_sepal_length\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bae602ac-c979-425d-ac20-7c983135209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the RDD: 151\n",
      "Sample lines: ['Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species', '1,5.1,3.5,1.4,0.2,Iris-setosa', '2,4.9,3.0,1.4,0.2,Iris-setosa', '3,4.7,3.2,1.3,0.2,Iris-setosa', '4,4.6,3.1,1.5,0.2,Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "rdd_lines = spark.sparkContext.textFile(\"/Users/abidhiafahmed/projets/realtime-spark/data/iris.csv\")\n",
    "\n",
    "print(\"Number of lines in the RDD:\", rdd_lines.count())\n",
    "print(\"Sample lines:\", rdd_lines.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c836071-b997-4fcf-915c-01715617102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words: 60\n",
      "Sample pairs (word, count): [('petallengthcm', 1), ('petalwidthcm', 1), ('setosa', 50), ('versicolor', 50), ('100', 1), ('102', 1), ('106', 1), ('107', 1), ('108', 1), ('110', 1)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_line(line: str) -> list[str]:\n",
    "    # Lowercase\n",
    "    line = line.lower()\n",
    "    # Replace non-alphanumeric characters with spaces\n",
    "    line = re.sub(r\"[^a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\", \" \", line)\n",
    "    # Split into words\n",
    "    words = line.split()\n",
    "    return words\n",
    "\n",
    "# 1. Split into words\n",
    "rdd_words_reviews = rdd_lines.flatMap(clean_line)\n",
    "\n",
    "# 2. Optional: filter very short words\n",
    "rdd_words_filtered = rdd_words_reviews.filter(lambda w: len(w) > 2)\n",
    "\n",
    "# 3. Map to (word, 1)\n",
    "rdd_pairs = rdd_words_filtered.map(lambda w: (w, 1))\n",
    "\n",
    "# 4. Aggregate by word\n",
    "rdd_word_count = rdd_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# 5. Display a few results\n",
    "print(\"Number of distinct words:\", rdd_word_count.count())\n",
    "print(\"Sample pairs (word, count):\", rdd_word_count.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c8db18-65a6-47f8-9f9d-1ecb6c5e2ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent words:\n",
      "iris : 150\n",
      "setosa : 50\n",
      "versicolor : 50\n",
      "virginica : 50\n",
      "petallengthcm : 1\n",
      "petalwidthcm : 1\n",
      "100 : 1\n",
      "102 : 1\n",
      "106 : 1\n",
      "107 : 1\n",
      "108 : 1\n",
      "110 : 1\n",
      "111 : 1\n",
      "112 : 1\n",
      "113 : 1\n",
      "115 : 1\n",
      "116 : 1\n",
      "119 : 1\n",
      "121 : 1\n",
      "122 : 1\n"
     ]
    }
   ],
   "source": [
    "rdd_count_word = rdd_word_count.map(lambda pair: (pair[1], pair[0]))\n",
    "\n",
    "rdd_sorted = rdd_count_word.sortByKey(ascending=False)\n",
    "\n",
    "top_20 = rdd_sorted.take(20)\n",
    "\n",
    "print(\"Top 20 most frequent words:\")\n",
    "for count, word in top_20:\n",
    "    print(f\"{word} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cd37aec-e8d4-4902-a1eb-8c4219a13d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in rdd_lines: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in rdd_lines:\", rdd_lines.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60f71e57-95e6-4a45-8a5f-4b6147be7b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions after repartition(8): 8\n"
     ]
    }
   ],
   "source": [
    "rdd_repart_8 = rdd_lines.repartition(8)\n",
    "print(\"Partitions after repartition(8):\", rdd_repart_8.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03445b33-1c0d-4487-b72f-b67ffb01f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions after coalesce(2): 2\n"
     ]
    }
   ],
   "source": [
    "rdd_coalesce_2 = rdd_lines.coalesce(2)\n",
    "print(\"Partitions after coalesce(2):\", rdd_coalesce_2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "469fb9aa-4bbf-49da-92e5-fad68a62bb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count with default partitioning -> result = 151, time = 0.0141 s\n",
      "count after repartition(8) -> result = 151, time = 0.0523 s\n",
      "count after coalesce(2) -> result = 151, time = 0.0141 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_time(action_fn, description: str):\n",
    "    start = time.time()\n",
    "    result = action_fn()\n",
    "    end = time.time()\n",
    "    print(f\"{description} -> result = {result}, time = {end - start:.4f} s\")\n",
    "\n",
    "# Compare count across different versions\n",
    "measure_time(lambda: rdd_lines.count(), \"count with default partitioning\")\n",
    "measure_time(lambda: rdd_repart_8.count(), \"count after repartition(8)\")\n",
    "measure_time(lambda: rdd_coalesce_2.count(), \"count after coalesce(2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26c904f9-1766-4933-8ade-89f2d96d0c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperationException",
     "evalue": "Cannot change storage level of an RDD after it was already assigned a level",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m rdd_words_filtered_persist \u001b[38;5;241m=\u001b[39m rdd_words_filtered\u001b[38;5;241m.\u001b[39mpersist(StorageLevel\u001b[38;5;241m.\u001b[39mMEMORY_AND_DISK)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Cache\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m rdd_words_filtered_cache \u001b[38;5;241m=\u001b[39m \u001b[43mrdd_words_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projets/realtime-spark/.venv/lib/python3.9/site-packages/pyspark/rdd.py:441\u001b[0m, in \u001b[0;36mRDD.cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03mPersist this RDD with the default storage level (`MEMORY_ONLY`).\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m>>> _ = rdd.unpersist()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStorageLevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMEMORY_ONLY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/projets/realtime-spark/.venv/lib/python3.9/site-packages/pyspark/rdd.py:509\u001b[0m, in \u001b[0;36mRDD.persist\u001b[0;34m(self, storageLevel)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    508\u001b[0m javaStorageLevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_getJavaStorageLevel(storageLevel)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjavaStorageLevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/projets/realtime-spark/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/projets/realtime-spark/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m: Cannot change storage level of an RDD after it was already assigned a level"
     ]
    }
   ],
   "source": [
    "# Intermediate RDD: filtered words\n",
    "rdd_words_filtered = rdd_words_reviews.filter(lambda w: len(w) > 2)\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "rdd_words_filtered_persist = rdd_words_filtered.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# Cache\n",
    "rdd_words_filtered_cache = rdd_words_filtered.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f116958-7445-44c2-b01c-3de1c8715376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First action\n",
    "print(\"Number of filtered words:\", rdd_words_filtered_cache.count())\n",
    "\n",
    "# Second action\n",
    "print(\"Sample filtered words:\", rdd_words_filtered_cache.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794303e-5178-45fa-bd7c-d345864d771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "rdd_words_filtered_persist = rdd_words_filtered.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b8e34-0b53-4b59-b2e3-d0eaa18c121f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
